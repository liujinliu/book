<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
作者：Michael Nielsen
英文在线：http://neuralnetworksanddeeplearning.com/index.html

# 使用神经网络识别手写数字
 人类的视觉系统是世界上最美妙的系统之一。考虑下面的手写数字  
 <!--more-->  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/1?raw=true)  
大多数人可以很快的识别出来是504192。这件事情很大程度上迷惑了人类，事实上这件事情远没有看起来这么简单。我们的大脑每个半球都包含一层主要的视觉感知皮层，我们可以把这一层称为V1，这一层包含超过1.4亿的神经元，并且他们彼此之间的连接数超过10亿。不仅如此，整个识别过程，除了V1层的处理外，还会涉及V2,V3,V4和V5，做着一步一步更复杂的图片处理。在上百万年的进化中，人类最终实现了每个人都自带一台超级计算机，并用来理解世界——识别手写数字才得以看起来毫不费力。我们在接受识别视觉信息方面是如此擅长，所有这一切就像是无意识完成的，我们甚至不会因此夸奖我们的视觉系统做了一件多么复杂的事情。
如果我们尝试写个电脑程序来识别上面的手写数字，就会发现视觉识别是一件多么复杂的事情。直觉上我们认为可以通过形状来判断，“9就是上面一个圆，然后右边下来有个钩”，这种表述在算法实现的难度暂且不说，当年尝试把这些规则更加细化，你会发现自己需要处理各种特殊情况，常常会掉入异常的沼泽地，整件事情走向绝望。
神经网络尝试采用一种不同的方式解决这个问题，首先是采用足够多的手写数字作为训练标本.  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/2?raw=true)  
然后开发一个系统，可以从这些标本里得到训练。换句话说，神经系统从这些标本中自动进化出一套识别手写数字到数字的对应规则。并且随着训练标本的增多，神经系统可以被训练的更准确的识别手写数字。这上面只列出来100个标本，我们实际上可以准备更多，上千个，上亿个。
这一章我们将会开发一个电脑程序，实现一个神经网络来识别手写数字。这个程序只有74行，没有使用任何开源框架，但是对手写数字的识别准确率可以达到96%以上。并且在接下来的章节，我们将改进策略，准确率可以达到99%以上。事实上, 好的商业技术已经被广泛被银行应用来进行支票的识别, 邮局则使用这种技术进行地址的识别.
我们聚焦在识别手写数字这个问题是因为这是一个认识神经网络的非常棒的原型问题. 这个问题首先具有一定的复杂性---识别手写数字不是一个简单的事情, 但同时这个问题又不需要很复杂的解决方案或是需要一个强大的计算机. 本书中我们将不断的回顾这个问题, 持续探讨一些更好的方法改进算法. 最后我们会讨论深度神经网络在诸如自然语言处理领域的应用. 
这一章如果只是介绍如何写一份代码来实现对手写数字的识别的话, 那么内容一定短的多. 实际上, 在这个过程中, 我们会不断的介绍一些关于神经网络方面的概念. 包括两个重要的人工神经的建模(感知器和S型神经网络). 并且介绍一种在神经网络中使用的典型的学习算法---随机梯度下降算法. 在本章的末尾, 针对本章提供的理论, 我会尽量做到让读者知其然, 知其所以然.
## 预测器
什么是神经网络? 为了开始介绍这些, 我会首先解释一个人工神经---预测器. 预测器在上世纪50年代和60年代由Frank Rosenblatt提出, 他同时受到了更早时候Warren McCulloch和Walter Pitts的工作的启发. 今天, 更多时候我们使用另外的模型来对神经网络建模---其中最主要的一种被称为S型神经网络. 晚些我们会研究S型神经网络, 但是首先让我们花点时间来了解下预测器, 这同时可以帮助我们认识为什么我们选择S型神经网络. 
预测器是如何工作的呢? 一个预测器接受一些二进制的输入\\(x_1,x_2,......\\), 并且得到一个二进制的输出:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/3.png?raw=true)  
上面这个例子所表示的预测器包含三个输入,\\(x_1,x_2,x_3\\). 一般来说预测器的模型并没有规定输入的个数. Rosenblatt提出了一种简单的规则来计算预测器的输出.他引入了权重的概念,\\(w_1,w_2,w_3\\), 权重表明了对应的输入对输出结果的影响力. 神经元的输出, 0或者1, 取决于加权和\\(\sum_jw_jx_j\\)与门限值的比较结果.门限值同样是神经元的一个参数, 下面的代数表达或许更准确些:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/4.png?raw=true)  
以上就是一个预测器工作的全部内容.
这是一个基本的数学模型. 你可以理解预测器其实就是根据一些证据来作出一个决策. 我来举个例子, 跟现实可能不是很符合, 但却有助于理解这里提到的模型(我们接下来将会讲更现实一点的模型). 假设周末就要来了, 并且你听说你的城市里将会有一个奶酪节, 你喜欢奶酪. 但你是否一定会去还要取决下面的几个条件:

1. 天气好么
2. 你的男(女)朋友是否愿意陪你一起
3. 奶酪节的地点距离地铁近么, 因为你没有车

我们可以采用三个参数\\(x_1,x_2,x_3\\)来表示这三种因子, \\(x_1=1\\)表示天气好, \\(x_1=0\\)表示天气糟糕, \\(x_2=1\\)表示你的女(男)朋友愿意陪你一起, \\(x_2=0\\)表示他不愿意,\\(x_3=1\\)表示那附近有地铁, \\(x_3=0\\)表示天那附近没有地铁.  
现在假设你非常喜欢奶酪, 即使没人陪你或是那附近没有地铁你一样愿意去, 但可能确实很在意天气, 因为如果天气很糟糕, 你真的没有办法过去. 你可以使用预测器来为你的这次决定建模, 权重分别为\\(w_1=6,w_2=2,w_3=2\\), 权重越大表明这个条件所占的比重越大, 然后假设你选择的门限值是5. 你可以看到, 只要天气好, 这个预测器的输出一定是1, 天气不好, 这个预测器的输出一定是0.  
通过修改这个门限, 我们可以得到不同的结果, 比如我们将门限降到3, 那么如果天气好, 或者地点附近有地铁并且你的女(男)朋友愿意陪你, 你都会去. 换句话说, 降低门限之后我们得到了一个新的预测器, 而这个预测器的模型表明了更强烈的去的欲望.  
显然, 预测器模型没有完整的展示我们人类做决定的过程. 但这个例子还是展示了对不同的输入因子施以不同的权重是如何影响最终的输出的. 让人高兴的是, 更复杂的预测器网络可以提供更微妙的决策输出.  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/5.png?raw=true)  
上面所示的网络中, 第一列预测器---我们称其为预测器的第一层---会根据输入生成三个输出(决策). 但是第二层是做什么的呢? 通过对第一层的输出(也即时第二层的输入)的加权运算, 得到一个新的输出(决策). 通过这种方式, 第二层预测器可以在一个更复杂更抽象的层面上工作. 然后, 第三层在此基础上可以作出更复杂的决策. 通过这种方式, 多层预测器可以从事更加复杂精密的预测.  
顺便说一句, 当我定义预测器的时候, 我说了预测器只有一个输出. 在上面这个网络, 看起来是有多个输出的. 事实上, 它依然是一个输出, 多个箭头仅仅是为了表明预测器的输出被用作多个预测器的输入.  
让我们来简化下对预测器的描述. 公式\\(\sum_jw_jx_j > threshold\\)显得有些笨重, 我们可以引入一些符号来简化这个表达式. 首先, 使用点乘, \\(w\cdot x \equiv \sum_jw_jx_j\\), \\(w\\)和\\(x\\)都是向量, 元素是相应的权重和输入. 接下来将门限值移到等式的另一边. 定义 \\(b \equiv -threshold\\)作为预测器的偏置. 于是, 预测器可以被重写为:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/6.png?raw=true)  
你可以认为这个偏置的大小表明了使预测器输出1的难易程度. 或者, 从更生物学的角度做个比较, 越大的偏置, 激起预测器的欲火就更容易. 对一个预测器来说, 给一个足够大的偏置, 则预测器很容易就可以输出1, 相反, 如果偏置很小, 甚至是负数, 那么显然使预测器输出1会难度很大. 引入偏置对于描述预测器来说只是一个很小的改动, 但接下来我们将看到这将给表达式带来极大的简化. 因此, 接下来, 我们不再使用门限值这个说法, 统一用偏置来代替(原文叫bias).  
我之前将预测器描述为通过对输入进行加权运算来得出决策的一种方式. 另外一种描述方式是用预测器来表示逻辑电路, 比如与们, 非门, 与非门. 比如下面这个预测器, 它具有两个输入, 相应的权重都是-2, 全局的偏置是3.  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/7.png?raw=true)  
显然上面这个预测器输入为0,0/0,1/1,0时候输出都是1, 但是输入1,1时候输出为0. 我们用预测器实现了一个与非门.  
上面这个与非门的例子表明了我们可以用预测器实现简单的逻辑功能. 实际上我们可以用预测器实现任意的逻辑功能. 那是因为与非门的组合可以实现任意的逻辑功能. 比如我们可以使用与非门实现两个bit的相加. 这需要实现按位和的功能\\(x_1 \oplus x_2\\), 同时, 为了实现进位计算,我们还需要实现按位乘\\(x_1x_2\\).  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/8.png?raw=true)  
使用两输入的预测器替代与非门, 权重设置为-2, 偏执设置为3, 我们可以得到一个预测器的网络.  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/9.png?raw=true)  
一个需要注意的地方是上面这个网络, 最左边预测器的输出被当作最下面的预测器的输入被使用了两次.当我定义预测器时候我并没有说这种情况是否允许, 事实上这样做是没有问题的. 当然如果你不想看到这样, 我们大可以将那两条线合并成一条, 但这样权重就变为-4, 像下面这样:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/10.png?raw=true)  
实际上, 我们还可以多画一层预测器, 像下面这样  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/11.png?raw=true)  
这种画法, 有输出没输入, 只是我们做的一个简化记号, 并不意味着一个没有输入的预测器.  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/12.png?raw=true)  
想象一下, 如果真的有这样一个预测器, 那么显然\\(\sum_jw_jx_j\\)会衡等于0, 也就是说预测器的输出将完全取决于偏置. 因此输入预测器仅仅提供输入, 这样理解是比较合理的.  
上面的加法器的例子如何使用预测器来仿真一个包含多个与非门的电路. 然而因为与非门是可以应用于通用计算的, 所以预测器也可以用于通用的计算领域.  
这个结论既让人高兴也让人失望. 让人高兴的是预测器可以像其他计算设备一样具有强大的计算能力, 但让人失望的是看起来预测器不过是一个新的类型的与非门, 这可不是什么大新闻.  
然而, 这个实验依然揭示了一点, 就是我们可以在这个神经元基础上开发一种学习算法, 可以自适应的调整自己的权重和偏置. 权重和偏置并非由程序直接生成, 而且根据对外部仿真的响应来调整参数. 通过简单的学习去解决问题, 而不是像传统思想一样事先摆好一堆与非门, 我们的神经网络可以处理很多在传统逻辑上很难解决的问题.  

## S型神经网络
学习算法听起来棒极了. 但是我们应该如何为一个神经网络设计学习算法呢? 假设我们有一个预测器网络, 我们需要让他自学习并解决一些问题. 比如输入是一个手写数字扫描出来的原始图片的像素数据. 我们希望可以让预测器学习到合适的权重和偏置, 这样它的输出可以正确的识别这个手写数字. 为了看清这个学习算法是如何工作的, 假设我们在权重(或者偏置)上做一个很小的改动, 我们希望这个很小的改动同样可以反应在输出上发生了一个很小的改动. 一会儿我们就会发现, 这种特性使得学习成为可能. 并且这正是我们想要的(显然下面这个网络用来识别手写数字有点太天真了).  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/13.png?raw=true)  
如果真的是像我们描述的那样, 一个小的权重或是偏置的更改, 带来一个小的输出的更改, 那么我们就可以利用这一特性一点点达成我们的目标. 假设我们的网络错误的将8认成了9, 我们可以小幅修改权重或是偏置, 使得结果更偏向9一点, 重复这一操作, 网络就一步步得到了学习.  
然而问题是如果网络中存在预测器, 我们上面所说的将很难存在. 事实上, 一个小的权重或是偏置的更改可能完全将预测器的输出反转. 这个反转会使得网络的剩余部分的行为难以控制. 所以当我们好不容易让这个网络识别出9之后, 在其他数字上很可能已经无法控制并且一团糟糕.也许存在某些聪明的办法可以解决这个问题, 但目前为止我们找不到让预测器去学习的办法.  
我们可以解决这个问题---引入一个新的神经网络, Sigmoid型神经网络, 我接下来都称其为S型神经网络. S型神经网络跟预测器有点类似, 但是它可以保证一个小的权重或是偏置的变化仅仅会在输出上引起一个小的变化. 正是这一点使得S型神经网络可以进行学习进化.  
好的, 现在我来描述下S型神经网络, 这初看起来跟我之前描述的预测器是一样的.  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/14.png?raw=true)  
就像一个预测器, S型神经元同样包含一些输入\\(x_1,x_2,x_3......\\), 但不一定是0/1, 这些输入可以在0和1之间任意取值. 同样的, S性神经元同样包含一系列权重\\(w_1,w_2,w_3......\\)和一个全局的偏置\\(b\\). 但输出不是0/1, 而是一个Sigmoid函数所表示的结果\\(\sigma(w \cdot x+b)\\):  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/15.png?raw=true)  
写的更确切一点是像下面这样, 输入\\(x_1,x_2,x_3......\\), 权重\\(w_1,w_2,w_3.....\\)和偏置\\(b\\)的S型神经元的输出:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/16.png?raw=true)  
第一眼看上去, S型神经元与预测器差别很大. Sigmoid函数看起来有点晦涩. 事实上, S型神经元跟预测器有很多类似的地方, 上面的公式体现的不过是一个技术上的细节, 不应该成为我们理解上的障碍.  
为了理解S型神经元与预测器的相似之处, 假设\\(z \equiv w \cdot x+b\\)是一个很大的整数. 那么\\(e^{-z} \approx 0 \\) , \\(\sigma (z) \approx 1\\). 换句话说, \\(z=w \cdot x+b\\) 是一个很大的正数, 这个S型神经元的输出就会非常接近1.  反过来, 如果\\(z=w \cdot x+b\\)是一个很小的负数, 那么\\(e^{-z} \approx \infty\\), \\(\sigma (z) \approx 0\\). 以上的表现都使得S型神经元跟预测器有很多相似之处. 当然在大部分情况下\\(z=w \cdot x+b\\)根据模型的情况会有一个适度的大小, 这使得S型神经元跟预测器有较大的不同.  
也许你会问, \\(\sigma\\)的代数形式是什么样的? 我应该如何理解? 其实, \\(\sigma\\)的确切形式并不重要---真正重要的是当它被画出来时候的样子, 下面就是这个函数的图形:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/17.png?raw=true)  
相比于下面这个阶越函数, 这个显然更平滑一些.  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/18.png?raw=true)  
如果\\(\sigma\\)表现为阶越函数的话, 那么S型神经元就完全退化为预测器, 因为\\(w \cdot x+b\\)是正数还是负数直接决定了输出是1还是0. 事实上, 最终起作用的正是\\(\sigma\\)函数的平滑属性, 而并非它的确切形式. 平滑意味着在权重上和(或)偏置上小的改动\\(\Delta w_j, \Delta b\\)最终在输出上仅仅会带来一个小的改动\\(\Delta output\\). 他们之间近似遵循下面的公式:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/19.png?raw=true)  
上面的公式中, \\(\partial output/ \partial w_j\\) 和\\(\partial output / \partial b\\)表示对\\(w_j\\)和\\(b\\)的偏微分. 当然, 如果你不懂偏微分没关系. 上面的式子其实不过表明了一个很简单的东西. 即\\(\Delta output\\) 跟\\(\Delta w_j, \Delta b\\)是呈线性相关的. 线性特性使得挑选\\(\Delta w_j, \Delta b\\)以得到一个想要的在结果上的变化变得相对容易很多. 在回想下S型神经元与预测器有很多相似之处, 得出如何改变\\(\Delta w_j, \Delta b\\)变的更容易了. 
如果\\(\sigma\\)函数的表现形式不重要, 那我们为什么不使用公式(3)呢? 事实上, 在接下来的章节, 我们在有些时候会使用一些\\(f(w \cdot x)+b\\)其他的激活函数形式. 换其他激活函数影响的主要是公式(5)中偏微分的值. 接下来我们会看到, 当计算偏微分时, 使用\\(\sigma\\)函数将给我们带来简化. 因为指数在微分上有让人喜欢的特性. \\(\sigma\\)函数是一个在神经网络上普遍应用的函数, 并且也是本书大部分时候采用的函数.  
我们应该如何解释S型神经元的输出呢? 显然, 一个很大的不同是S型神经元输出不止0或1, 而是0和1之间的任何数字. 当我们想用输出表示输入到神经元的图片像素的平均密度, 会发现这个很有用. 但有时候这也会带来干扰. 假设我们希望输出告诉我们输入的图片是8还是9, 那么如果输出只有0或1这对我们就很友好了, 就像预测器干的那样. 当然, 针对这一点, 我们在S型神经元上引入一个判断机制会比较好一点, 比如输出大于0.5都表明输入的图片是9, 所有小与等于0.5都表示输入的图片不是9. 当前, 接下来讲的时候使用这个规则我都会提前表述, 相信不会带来误解.  
## 神经网络的架构
接下来这一节我会介绍一种神经网络, 它可以比较高效的识别手写数字. 在开始之前, 我们有必要解释一些术语, 假设我们有以下的神经网络:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/20.png?raw=true)  
最左边叫做输入层, 其中的神经元叫输入神经元, 最右边叫做输出层, 包含一个输出神经元. 中间的这层叫做隐藏层, 因为这一层神经元既不是输人也不是输出. "隐藏"这个词乍一听有点神秘---当我第一次听说的时候我也以为这一定表示什么高深的数学概念, 但相信我, 它仅仅表示这一层既不是输入也不是输出. 上面这个例子只包含一个隐藏层, 实际上有些, 甚至是大多数神经网络都有多层隐藏层, 下面这个四层神经网络就包含两个隐藏层:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/21.png?raw=true)  
因为历史原因, 这样的多层网络有时候也被称为多层预测器或是MLPs(multilayer perceptrons), 因为这个网络是由S型神经元组成的, 而不是预测器, 所以接下来我将会使用MLP这个术语, 因为我觉得这容易有些混淆, 所以在此特别说一下.  
网络的输入和输出设计通常都是很简单的.举个例子, 假设我们要判断是一个手写数字是否是9. 一种比较自然的方式是使用图片的像素点强度编码后的值作为神经元的输入. 如果图片是一个\\(64 \times 64\\)的灰度图, 那么我们将可以得到\\(4096=64 \times 64\\)个神经元, 输入值分布于0~1之间. 输出只包含一个神经元, 大于0.5表示这个图片是9, 小与0.5表示这个图片不是9.  
输入神经元和输出神经元的设计都是很简单直接的, 那么如何设计隐藏层就是个技术活了, 或者说是一件关乎艺术的事情. 特别的, 想使用几个简单的经验法则来设计隐藏层是不可能的. 为了让从神经网络获得想要的输出, 研究人员开发了很多设计方法. 例如有些方法帮助我们确定如何在网络训练时间和网络隐藏层的数量上取得均衡. 在晚些时候我们将看到这种设计方法.  
目前为止, 我们讨论的是前馈网络---前一层的神经元的输出将作为下一层的输入. 这意外着网络中没有环的存在, 信息流是一直向前流动的. 如果我们引入循环, 那么\\(\sigma\\)函数的输入将收到输出的影响, 这让人很难理解, 因此我们目前不允许这样的循环出现.  
然而, 现实中确实是存在包含反馈循环的人工神经网络的. 这种模型被称作[递归神经网络](https://en.wikipedia.org/wiki/Recurrent_neural_network). 关于递归神经网络这里就不展开了, 递归神经网络的影响力不如前馈神经网络, 部分原因是目前为止, 递归神经网络的算法没有前馈神经网络强大. 这里要说的一点是, 递归神经网络依然很有趣, 在模拟我们的大脑工作上, 递归神经网络显然更符合逻辑. 而且有些在前馈网络看来很难解决的问题, 可能是递归神经网络的用武之地. 但是本书将不会展开去讨论递归神经网络, 而主要关注点在应用更广泛的前馈神经网络上.  
## 一个识别手写数字的简单网络
我们将识别手写数字分成两个子问题. 首先我们可以将包含一串数字的图片分成一个一个的包含一个数字的图片. 比如下面的图片可以被分解:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/22.png?raw=true)  
分解之后为6张独立图片:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/23.png?raw=true)  
分成6张独立图片对我们人类来说很简单, 但对机器来说却有不小的挑战. 一旦数字被分解成独立的, 接下来就需要程序识别出单个图片, 比如, 我们需要我们的机器识别出下面这个张图片是一个5  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/24.png?raw=true)  
我们将把主要精力放在第二个问题上, 即识别单个图片. 这是因为, 相信我, 当你可以做到第二点的时候, 第一个不会再有什么难度. 为了识别单个手写数字, 我们使用下面的三层网络:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/25.png?raw=true)  
网络的第一层神经元是对输入像素值的编码. 我们下一节要讨论的, 将使用28*28像素的扫描手写数字图片. 因此输入层包含\\(784=28 \times 28\\)个神经元. 显然我上图没有画这么多输入神经元, 这当然是为了简化, 而且希望你可以谅解. 输入像素是灰度图, 0表示白色, 1表示黑色, 0到1至今的数值表明灰色(从浅到深).  
第二层是隐藏层, 我们用n来表示隐藏层的神经元个数. 我们将尝试使用不同的n, 上面这个例子使用了15.  
网络的输出层采用10个神经元. 如果第一个神经元非常接近1, 那么表示我们这个网络推测出来这个图片代表的是0. 如果第二个神经元输出非常接近1, 那么我们说这个网络推测出来这个数字是1. 更准确一点说, 输出神经元分别代表了输入图片是0~9的数字的概率, 预测结果就是输出最大的那个神经元所代表的数字. 比如输出最大的是第6个神经元, 那么这个网络就是预测输入的图片表示的是6.  
你可能会想, 为什么我们输出采用10个, 因为如果每一个输出是一个二进制数的话, 那么4bit就足够了, 因为\\(2^4 = 16\\), 而我们实际上只想对应10个输出结果. 这样看来, 似乎我们的网络有些效率低了, 但其实这是经验得来的数据, 采用10个输出比采用4个输出效果会更好. 但你可能不满足于知其然, 接下来我将简单解释下.  
为了理解我们这么做的原因, 我们需要从最基础的说起. 首先考虑10个输出的情况. 让我们关注第一个输出神经元, 它用来判断输入的图片是否是0. 它通过权衡隐藏层的结果来作出判断. 但是隐藏层究竟都做了什么呢? 我们假设隐藏层的第一个神经元判断图片是否存在下图所示的内容:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/26.png?raw=true)  
它可以通过在分配加权值时对与图片中阴影重合部分加大加权值, 而对其他非阴影部分采用较小加权值来实现. 同样的, 我们假设隐藏层的第二, 第三, 第四个神经元是为了判断下面的特征是否存在:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/27.png?raw=true)  
你也许已经猜到了, 这四个特征图片一起组成了0:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/28.png?raw=true)  
因此如果这四个特征图片都满足的话, 我们说输入的图片是0. 当然这不是我们判断图片是0的唯一途径, 事实上, 通过对上面的特征进行稍微的改动, 比如方向上的, 或是稍微扭曲下, 我们都可以认为输入的图片是0. 但至少在这种情况下, 我们可以肯定的说输入图片是0.   
假设网络按照我们上面描述的那样工作, 那么我们就可以相对简单的解释为什么选择10个输出神经元而不是4个. 因为如果选择4个输出神经元, 很难想象能有什么算法让输入对应到具体的bit位上.  
当然, 不排除有更聪明的算法使得使用4个输出也得到不错的结果, 但显然我这里描述的网络工作方式简化了很多工作. 这对你相信是有益的.  
## 梯度下降学习算法
现在我们已经有了一个设计好的网络, 我们应该如何训练它来识别手写数字呢? 首先要做的就是选择训练的数据集. 我们将使用[MINIST数据集合](http://yann.lecun.com/exdb/mnist/), 它包含成千上万的手写数字的扫描图像, 并且已经替我们做好了归类. 以下是它的一些图片:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/29.png?raw=true)  
你会发现这就是我们在文章开始处所引用的图片例子. 当然我们训练完这个网络后, 测试时候不会使用我们的训练数据.   
MINIST数据包含两部分, 第一部分包含60000个图像作为训练数据. 这些图像是250个人的手写数字的扫描, 其中一半是美国人口普查局的雇员, 还有一半是高中生. 这些图像都是灰度图, 大小是\\(28 \times 28\\)像素. 第二部分是10000个用来测试的图像. 也是同样大小.为了让测试更有说服力, 测试图片来自不同的250人, 尽管他们也是由美国人口普查局的雇员和高中生组成, 但他们是不同的人.  
我们是用\\(x\\)来表示输入. 每个输入是一个\\(28 \times 28 = 784\\)长度的向量. 每个元素代表了图片上一个灰度值. 我们定义输出为\\(y = y(x)\\), \\(y\\)是一个长度为10的向量. 现在假设有个训练图片是6, 那么我们希望的输出为\\(y(x) = (0,0,0,0,0,0,1,0,0,0)^T\\).我们需要一种算法得到合适的权重和偏置, 使得输入为\\(x\\)时, 输出尽可能接近\\(y(x)\\). 为了衡量这一点, 我们定义下面的成本函数:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/30.png?raw=true)  