<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
作者：Michael Nielsen
英文在线：http://neuralnetworksanddeeplearning.com/index.html

# 第一章
## 使用神经网络识别手写数字
 人类的视觉系统是世界上最美妙的系统之一。考虑下面的手写数字  
 <!--more-->  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/1?raw=true)  
大多数人可以很快的识别出来是504192。这件事情很大程度上迷惑了人类，事实上这件事情远没有看起来这么简单。我们的大脑每个半球都包含一层主要的视觉感知皮层，我们可以把这一层称为V1，这一层包含超过1.4亿的神经元，并且他们彼此之间的连接数超过10亿。不仅如此，整个识别过程，除了V1层的处理外，还会涉及V2,V3,V4和V5，做着一步一步更复杂的图片处理。在上百万年的进化中，人类最终实现了每个人都自带一台超级计算机，并用来理解世界——识别手写数字才得以看起来毫不费力。我们在接受识别视觉信息方面是如此擅长，所有这一切就像是无意识完成的，我们甚至不会因此夸奖我们的视觉系统做了一件多么复杂的事情。
如果我们尝试写个电脑程序来识别上面的手写数字，就会发现视觉识别是一件多么复杂的事情。直觉上我们认为可以通过形状来判断，“9就是上面一个圆，然后右边下来有个钩”，这种表述在算法实现的难度暂且不说，当年尝试把这些规则更加细化，你会发现自己需要处理各种特殊情况，常常会掉入异常的沼泽地，整件事情走向绝望。
神经网络尝试采用一种不同的方式解决这个问题，首先是采用足够多的手写数字作为训练标本.  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/2?raw=true)  
然后开发一个系统，可以从这些标本里得到训练。换句话说，神经系统从这些标本中自动进化出一套识别手写数字到数字的对应规则。并且随着训练标本的增多，神经系统可以被训练的更准确的识别手写数字。这上面只列出来100个标本，我们实际上可以准备更多，上千个，上亿个。
这一章我们将会开发一个电脑程序，实现一个神经网络来识别手写数字。这个程序只有74行，没有使用任何开源框架，但是对手写数字的识别准确率可以达到96%以上。并且在接下来的章节，我们将改进策略，准确率可以达到99%以上。事实上, 好的商业技术已经被广泛被银行应用来进行支票的识别, 邮局则使用这种技术进行地址的识别.
我们聚焦在识别手写数字这个问题是因为这是一个认识神经网络的非常棒的原型问题. 这个问题首先具有一定的复杂性---识别手写数字不是一个简单的事情, 但同时这个问题又不需要很复杂的解决方案或是需要一个强大的计算机. 本书中我们将不断的回顾这个问题, 持续探讨一些更好的方法改进算法. 最后我们会讨论深度神经网络在诸如自然语言处理领域的应用. 
这一章如果只是介绍如何写一份代码来实现对手写数字的识别的话, 那么内容一定短的多. 实际上, 在这个过程中, 我们会不断的介绍一些关于神经网络方面的概念. 包括两个重要的人工神经的建模(感知器和S型神经网络). 并且介绍一种在神经网络中使用的典型的学习算法---随机梯度下降算法. 在本章的末尾, 针对本章提供的理论, 我会尽量做到让读者知其然, 知其所以然.
## 预测器
什么是神经网络? 为了开始介绍这些, 我会首先解释一个人工神经---预测器. 预测器在上世纪50年代和60年代由Frank Rosenblatt提出, 他同时受到了更早时候Warren McCulloch和Walter Pitts的工作的启发. 今天, 更多时候我们使用另外的模型来对神经网络建模---其中最主要的一种被称为S型神经网络. 晚些我们会研究S型神经网络, 但是首先让我们花点时间来了解下预测器, 这同时可以帮助我们认识为什么我们选择S型神经网络. 
预测器是如何工作的呢? 一个预测器接受一些二进制的输入\\(x_1,x_2,......\\), 并且得到一个二进制的输出:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/3.png?raw=true)  
上面这个例子所表示的预测器包含三个输入,\\(x_1,x_2,x_3\\). 一般来说预测器的模型并没有规定输入的个数. Rosenblatt提出了一种简单的规则来计算预测器的输出.他引入了权重的概念,\\(w_1,w_2,w_3\\), 权重表明了对应的输入对输出结果的影响力. 神经元的输出, 0或者1, 取决于加权和\\(\sum_jw_jx_j\\)与门限值的比较结果.门限值同样是神经元的一个参数, 下面的代数表达或许更准确些:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/4.png?raw=true)  
以上就是一个预测器工作的全部内容.
这是一个基本的数学模型. 你可以理解预测器其实就是根据一些证据来作出一个决策. 我来举个例子, 跟现实可能不是很符合, 但却有助于理解这里提到的模型(我们接下来将会讲更现实一点的模型). 假设周末就要来了, 并且你听说你的城市里将会有一个奶酪节, 你喜欢奶酪. 但你是否一定会去还要取决下面的几个条件:

1. 天气好么
2. 你的男(女)朋友是否愿意陪你一起
3. 奶酪节的地点距离地铁近么, 因为你没有车

我们可以采用三个参数\\(x_1,x_2,x_3\\)来表示这三种因子, \\(x_1=1\\)表示天气好, \\(x_1=0\\)表示天气糟糕, \\(x_2=1\\)表示你的女(男)朋友愿意陪你一起, \\(x_2=0\\)表示他不愿意,\\(x_3=1\\)表示那附近有地铁, \\(x_3=0\\)表示天那附近没有地铁.  
现在假设你非常喜欢奶酪, 即使没人陪你或是那附近没有地铁你一样愿意去, 但可能确实很在意天气, 因为如果天气很糟糕, 你真的没有办法过去. 你可以使用预测器来为你的这次决定建模, 权重分别为\\(w_1=6,w_2=2,w_3=2\\), 权重越大表明这个条件所占的比重越大, 然后假设你选择的门限值是5. 你可以看到, 只要天气好, 这个预测器的输出一定是1, 天气不好, 这个预测器的输出一定是0.  
通过修改这个门限, 我们可以得到不同的结果, 比如我们将门限降到3, 那么如果天气好, 或者地点附近有地铁并且你的女(男)朋友愿意陪你, 你都会去. 换句话说, 降低门限之后我们得到了一个新的预测器, 而这个预测器的模型表明了更强烈的去的欲望.  
显然, 预测器模型没有完整的展示我们人类做决定的过程. 但这个例子还是展示了对不同的输入因子施以不同的权重是如何影响最终的输出的. 让人高兴的是, 更复杂的预测器网络可以提供更微妙的决策输出.  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/5.png?raw=true)  
上面所示的网络中, 第一列预测器---我们称其为预测器的第一层---会根据输入生成三个输出(决策). 但是第二层是做什么的呢? 通过对第一层的输出(也即时第二层的输入)的加权运算, 得到一个新的输出(决策). 通过这种方式, 第二层预测器可以在一个更复杂更抽象的层面上工作. 然后, 第三层在此基础上可以作出更复杂的决策. 通过这种方式, 多层预测器可以从事更加复杂精密的预测.  
顺便说一句, 当我定义预测器的时候, 我说了预测器只有一个输出. 在上面这个网络, 看起来是有多个输出的. 事实上, 它依然是一个输出, 多个箭头仅仅是为了表明预测器的输出被用作多个预测器的输入.  
让我们来简化下对预测器的描述. 公式\\(\sum_jw_jx_j > threshold\\)显得有些笨重, 我们可以引入一些符号来简化这个表达式. 首先, 使用点乘, \\(w\cdot x \equiv \sum_jw_jx_j\\), \\(w\\)和\\(x\\)都是向量, 元素是相应的权重和输入. 接下来将门限值移到等式的另一边. 定义 \\(b \equiv -threshold\\)作为预测器的偏置. 于是, 预测器可以被重写为:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/6.png?raw=true)  
你可以认为这个偏置的大小表明了使预测器输出1的难易程度. 或者, 从更生物学的角度做个比较, 越大的偏置, 激起预测器的欲火就更容易. 对一个预测器来说, 给一个足够大的偏置, 则预测器很容易就可以输出1, 相反, 如果偏置很小, 甚至是负数, 那么显然使预测器输出1会难度很大. 引入偏置对于描述预测器来说只是一个很小的改动, 但接下来我们将看到这将给表达式带来极大的简化. 因此, 接下来, 我们不再使用门限值这个说法, 统一用偏置来代替(原文叫bias).  
我之前将预测器描述为通过对输入进行加权运算来得出决策的一种方式. 另外一种描述方式是用预测器来表示逻辑电路, 比如与们, 非门, 与非门. 比如下面这个预测器, 它具有两个输入, 相应的权重都是-2, 全局的偏置是3.  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/7.png?raw=true)  
显然上面这个预测器输入为0,0/0,1/1,0时候输出都是1, 但是输入1,1时候输出为0. 我们用预测器实现了一个与非门.  
上面这个与非门的例子表明了我们可以用预测器实现简单的逻辑功能. 实际上我们可以用预测器实现任意的逻辑功能. 那是因为与非门的组合可以实现任意的逻辑功能. 比如我们可以使用与非门实现两个bit的相加. 这需要实现按位和的功能\\(x_1 \oplus x_2\\), 同时, 为了实现进位计算,我们还需要实现按位乘\\(x_1x_2\\).  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/8.png?raw=true)  
使用两输入的预测器替代与非门, 权重设置为-2, 偏执设置为3, 我们可以得到一个预测器的网络.  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/9.png?raw=true)  
一个需要注意的地方是上面这个网络, 最左边预测器的输出被当作最下面的预测器的输入被使用了两次.当我定义预测器时候我并没有说这种情况是否允许, 事实上这样做是没有问题的. 当然如果你不想看到这样, 我们大可以将那两条线合并成一条, 但这样权重就变为-4, 像下面这样:  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/10.png?raw=true)  
实际上, 我们还可以多画一层预测器, 像下面这样  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/11.png?raw=true)  
这种画法, 有输出没输入, 只是我们做的一个简化记号, 并不意味着一个没有输入的预测器.  
![这里写图片描述](https://github.com/liujinliu/book/blob/master/Neural-Networks-and-Deep-Learning/ChapterOne/img/12.png?raw=true)  
想象一下, 如果真的有这样一个预测器, 那么显然\\(\sum_jw_jx_j\\)会衡等于0, 也就是说预测器的输出将完全取决于偏置. 因此输入预测器仅仅提供输入, 这样理解是比较合理的.  
上面的加法器的例子如何使用预测器来仿真一个包含多个与非门的电路. 然而因为与非门是可以应用于通用计算的, 所以预测器也可以用于通用的计算领域.  
这个结论既让人高兴也让人失望. 让人高兴的是预测器可以像其他计算设备一样具有强大的计算能力, 但让人失望的是看起来预测器不过是一个新的类型的与非门, 这可不是什么大新闻.  
然而, 这个实验依然揭示了一点, 就是我们可以在这个神经元基础上开发一种学习算法, 可以自适应的调整自己的权重和偏置. 权重和偏置并非由程序直接生成, 而且根据对外部仿真的响应来调整参数. 通过简单的学习去解决问题, 而不是像传统思想一样事先摆好一堆与非门, 我们的神经网络可以处理很多在传统逻辑上很难解决的问题.  